Index: UTILS/dataTraining.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pandas as pd\r\nimport numpy as np\r\nfrom sklearn.preprocessing import MinMaxScaler\r\nfrom tensorflow.keras.models import Sequential\r\nfrom tensorflow.keras.layers import LSTM, Dense, Input\r\nfrom tensorflow.keras.callbacks import EarlyStopping\r\nfrom sklearn.metrics import mean_absolute_error\r\n\r\n# Load and prepare data\r\ndf = pd.read_csv(\"Updated_With_Boiler_Hourly_Realistic_v4.csv\", parse_dates=[\"date\"])\r\ndf = pd.get_dummies(df, columns=[\"weather_description\"])\r\n\r\nbase_features = [\r\n    \"temperature_2m\", \"relative_humidity_2m\", \"dew_point_2m\", \"apparent_temperature\",\r\n    \"precipitation\", \"cloud_cover\", \"wind_speed_10m\", \"is_day\",\r\n    \"direct_radiation\", \"surface_pressure\", \"weather_code\"\r\n]\r\none_hot_features = [col for col in df.columns if col.startswith(\"weather_description_\")]\r\nfeatures = base_features + one_hot_features\r\n\r\ntargets = [\r\n    \"boiler temp for 50 L with solar system\",\r\n    \"boiler temp for 50 L without solar system\",\r\n    \"boiler temp for 100 L with solar system\",\r\n    \"boiler temp for 100 L without solar system\",\r\n    \"boiler temp for 150 L with solar system\",\r\n    \"boiler temp for 150 L without solar system\"\r\n]\r\n\r\n# Drop NA\r\ndf = df.dropna(subset=features + targets).reset_index(drop=True)\r\n\r\n# Add day column\r\ndf[\"day\"] = df[\"date\"].dt.date\r\n\r\n# Split train / val / test by unique days\r\nunique_days = df[\"day\"].unique()\r\nn_days = len(unique_days)\r\ntrain_days = unique_days[:int(n_days * 0.7)]\r\nval_days = unique_days[int(n_days * 0.7):int(n_days * 0.85)]\r\ntest_days = unique_days[int(n_days * 0.85):]\r\n\r\ntrain_df = df[df[\"day\"].isin(train_days)].drop(columns=\"day\")\r\nval_df = df[df[\"day\"].isin(val_days)].drop(columns=\"day\")\r\ntest_df = df[df[\"day\"].isin(test_days)].drop(columns=\"day\")\r\n\r\n# Normalize inputs\r\nscaler_x = MinMaxScaler()\r\nX_train = scaler_x.fit_transform(train_df[features])\r\nX_val = scaler_x.transform(val_df[features])\r\nX_test = scaler_x.transform(test_df[features])\r\n\r\nX_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\r\nX_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\r\nX_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\r\n\r\n# Normalize outputs\r\nscaler_y = MinMaxScaler()\r\ny_train = scaler_y.fit_transform(train_df[targets])\r\ny_val = scaler_y.transform(val_df[targets])\r\ny_test_actual = test_df[targets].values\r\n\r\n# Define and train model\r\nmodel = Sequential([\r\n    Input(shape=(1, X_train.shape[2])),\r\n    LSTM(50, activation='relu'),\r\n    Dense(len(targets))\r\n])\r\nmodel.compile(optimizer='adam', loss='mse')\r\n\r\nearly_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\r\n\r\nmodel.fit(\r\n    X_train, y_train,\r\n    epochs=50,\r\n    validation_data=(X_val, y_val),\r\n    callbacks=[early_stop],\r\n    verbose=1\r\n)\r\n\r\n# Save the trained model\r\nmodel.save(\"boiler_temperature_multitarget.h5\")\r\n\r\n# Predict and inverse-scale\r\ny_pred_scaled = model.predict(X_test)\r\ny_pred = scaler_y.inverse_transform(y_pred_scaled)\r\n\r\n# Create results DataFrame\r\ndf_result = pd.DataFrame({\r\n    \"time\": test_df[\"date\"].values\r\n})\r\nfor i, target in enumerate(targets):\r\n    df_result[f\"{target} - Actual\"] = y_test_actual[:, i]\r\n    df_result[f\"{target} - Predicted\"] = y_pred[:, i]\r\n    df_result[f\"{target} - Error %\"] = 100 * np.abs(y_pred[:, i] - y_test_actual[:, i]) / y_test_actual[:, i]\r\n\r\n# Add hour column for grouping\r\ndf_result[\"hour\"] = pd.to_datetime(df_result[\"time\"]).dt.floor(\"h\")\r\n\r\n# Compute per-hour summary\r\nsummary_rows = []\r\nfor target in targets:\r\n    hourly = df_result.groupby(\"hour\").agg({\r\n        f\"{target} - Actual\": \"mean\",\r\n        f\"{target} - Error %\": \"mean\"\r\n    }).rename(columns={\r\n        f\"{target} - Actual\": \"Average Actual Temp\",\r\n        f\"{target} - Error %\": \"Mean Error %\"\r\n    }).reset_index()\r\n    hourly[\"Target\"] = target\r\n    summary_rows.append(hourly)\r\n\r\nsummary_df = pd.concat(summary_rows, ignore_index=True)\r\n\r\n# Export results\r\ndf_result.drop(columns=[\"hour\"]).to_csv(\"boiler_multitarget_predictions.csv\", index=False)\r\nsummary_df.to_csv(\"boiler_multitarget_summary.csv\", index=False)\r\n\r\nprint(\"‚úÖ Model saved as boiler_temperature_multitarget.h5\")\r\nprint(\"\uD83D\uDCC1 CSV files created: boiler_multitarget_predictions.csv + boiler_multitarget_summary.csv\")\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/UTILS/dataTraining.py b/UTILS/dataTraining.py
--- a/UTILS/dataTraining.py	(revision 962acd81f45087c04b5cf28a31575b8ba5585d4d)
+++ b/UTILS/dataTraining.py	(date 1744296917129)
@@ -1,24 +1,18 @@
 import pandas as pd
 import numpy as np
+import time
 from sklearn.preprocessing import MinMaxScaler
 from tensorflow.keras.models import Sequential
 from tensorflow.keras.layers import LSTM, Dense, Input
 from tensorflow.keras.callbacks import EarlyStopping
-from sklearn.metrics import mean_absolute_error
 
-# Load and prepare data
+start_time = time.time()
+
+# === 1. Load CSV ===
 df = pd.read_csv("Updated_With_Boiler_Hourly_Realistic_v4.csv", parse_dates=["date"])
-df = pd.get_dummies(df, columns=["weather_description"])
 
-base_features = [
-    "temperature_2m", "relative_humidity_2m", "dew_point_2m", "apparent_temperature",
-    "precipitation", "cloud_cover", "wind_speed_10m", "is_day",
-    "direct_radiation", "surface_pressure", "weather_code"
-]
-one_hot_features = [col for col in df.columns if col.startswith("weather_description_")]
-features = base_features + one_hot_features
-
-targets = [
+# === 2. Define targets ===
+target_columns = [
     "boiler temp for 50 L with solar system",
     "boiler temp for 50 L without solar system",
     "boiler temp for 100 L with solar system",
@@ -27,13 +21,26 @@
     "boiler temp for 150 L without solar system"
 ]
 
-# Drop NA
-df = df.dropna(subset=features + targets).reset_index(drop=True)
+# === 3. Define base features ===
+base_features = [
+    "temperature_2m", "relative_humidity_2m", "dew_point_2m", "apparent_temperature",
+    "precipitation", "cloud_cover", "wind_speed_10m", "is_day",
+    "direct_radiation", "surface_pressure", "weather_code", "weather_description"
+]
+
+# === 4. Drop NaNs ===
+df = df.dropna(subset=base_features + target_columns).reset_index(drop=True)
 
-# Add day column
+# === 5. Limit weather_description to top 10 ===
+top_k = 10
+top_weather = df["weather_description"].value_counts().nlargest(top_k).index
+df["weather_description"] = df["weather_description"].where(df["weather_description"].isin(top_weather), "Other")
+
+# === 6. One-hot encode weather_description only ===
+df = pd.get_dummies(df, columns=["weather_description"])
+
+# === 7. Split into train/val/test by day ===
 df["day"] = df["date"].dt.date
-
-# Split train / val / test by unique days
 unique_days = df["day"].unique()
 n_days = len(unique_days)
 train_days = unique_days[:int(n_days * 0.7)]
@@ -44,77 +51,95 @@
 val_df = df[df["day"].isin(val_days)].drop(columns="day")
 test_df = df[df["day"].isin(test_days)].drop(columns="day")
 
-# Normalize inputs
+# === 8. Create consistent feature list from TRAIN only ===
+columns_to_exclude = target_columns + ["date"]
+features = sorted([col for col in train_df.columns if col not in columns_to_exclude])
+
+# === 9. Add missing feature columns to val/test (with 0s)
+for col in features:
+    for part in [val_df, test_df]:
+        if col not in part.columns:
+            part[col] = 0.0
+
+# === 10. Normalize ===
 scaler_x = MinMaxScaler()
-X_train = scaler_x.fit_transform(train_df[features])
-X_val = scaler_x.transform(val_df[features])
-X_test = scaler_x.transform(test_df[features])
+X_train_raw = scaler_x.fit_transform(train_df[features])
+X_val_raw = scaler_x.transform(val_df[features])
+X_test_raw = scaler_x.transform(test_df[features])
 
-X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))
-X_val = X_val.reshape((X_val.shape[0], 1, X_val.shape[1]))
-X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))
-
-# Normalize outputs
 scaler_y = MinMaxScaler()
-y_train = scaler_y.fit_transform(train_df[targets])
-y_val = scaler_y.transform(val_df[targets])
-y_test_actual = test_df[targets].values
+y_train_raw = scaler_y.fit_transform(train_df[target_columns])
+y_val_raw = scaler_y.transform(val_df[target_columns])
+y_test_actual = test_df[target_columns].values
+
+# === 11. Rebuild scaled DataFrames
+train_df_scaled = pd.DataFrame(X_train_raw, columns=features)
+train_df_scaled[target_columns] = y_train_raw
+
+val_df_scaled = pd.DataFrame(X_val_raw, columns=features)
+val_df_scaled[target_columns] = y_val_raw
+
+test_df_scaled = pd.DataFrame(X_test_raw, columns=features)
+test_df_scaled[target_columns] = y_test_actual
+
+# === ‚úÖ 12. DEBUG: check alignment
+print("‚úÖ feature count:", len(features))
+print("‚úÖ train_df_scaled shape:", train_df_scaled.shape)
+
+for df_name, df_part in [("train", train_df_scaled), ("val", val_df_scaled), ("test", test_df_scaled)]:
+    df_cols = df_part.columns.tolist()
+    for col in features:
+        if col not in df_cols:
+            raise ValueError(f"‚ùå {col} missing in {df_name}_df_scaled")
+
+# === 13. Create sequences
+def create_sequences(df, feature_cols, target_cols, seq_len=6):
+    X_seq, y_seq = [], []
+    for i in range(seq_len, len(df)):
+        X_seq.append(df[feature_cols].iloc[i-seq_len:i].values)
+        y_seq.append(df[target_cols].iloc[i].values)
+    return np.array(X_seq), np.array(y_seq)
+
+SEQUENCE_LENGTH = 6
+X_train, y_train = create_sequences(train_df_scaled, features, target_columns, SEQUENCE_LENGTH)
+X_val, y_val = create_sequences(val_df_scaled, features, target_columns, SEQUENCE_LENGTH)
+X_test, y_test_actual_seq = create_sequences(test_df_scaled, features, target_columns, SEQUENCE_LENGTH)
 
-# Define and train model
+# === 14. Build and train LSTM model
+from tensorflow.keras.models import Sequential
+from tensorflow.keras.layers import LSTM, Dense, Input
+from tensorflow.keras.callbacks import EarlyStopping
+
 model = Sequential([
-    Input(shape=(1, X_train.shape[2])),
+    Input(shape=(SEQUENCE_LENGTH, X_train.shape[2])),
     LSTM(50, activation='relu'),
-    Dense(len(targets))
+    Dense(len(target_columns))
 ])
 model.compile(optimizer='adam', loss='mse')
-
 early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
 
-model.fit(
-    X_train, y_train,
-    epochs=50,
-    validation_data=(X_val, y_val),
-    callbacks=[early_stop],
-    verbose=1
-)
+model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=[early_stop], verbose=1)
 
-# Save the trained model
-model.save("boiler_temperature_multitarget.h5")
+# === 15. Save model
+model.save("boiler_temperature_multitarget_lstm6h.h5")
 
-# Predict and inverse-scale
+# === 16. Predict and inverse scale
 y_pred_scaled = model.predict(X_test)
 y_pred = scaler_y.inverse_transform(y_pred_scaled)
 
-# Create results DataFrame
+# === 17. Save predictions
 df_result = pd.DataFrame({
-    "time": test_df["date"].values
+    "time": test_df["date"].iloc[SEQUENCE_LENGTH:SEQUENCE_LENGTH+len(y_pred)].values
 })
-for i, target in enumerate(targets):
-    df_result[f"{target} - Actual"] = y_test_actual[:, i]
+for i, target in enumerate(target_columns):
+    df_result[f"{target} - Actual"] = y_test_actual_seq[:, i]
     df_result[f"{target} - Predicted"] = y_pred[:, i]
-    df_result[f"{target} - Error %"] = 100 * np.abs(y_pred[:, i] - y_test_actual[:, i]) / y_test_actual[:, i]
-
-# Add hour column for grouping
-df_result["hour"] = pd.to_datetime(df_result["time"]).dt.floor("h")
+    df_result[f"{target} - Error %"] = 100 * np.abs(y_pred[:, i] - y_test_actual_seq[:, i]) / y_test_actual_seq[:, i]
 
-# Compute per-hour summary
-summary_rows = []
-for target in targets:
-    hourly = df_result.groupby("hour").agg({
-        f"{target} - Actual": "mean",
-        f"{target} - Error %": "mean"
-    }).rename(columns={
-        f"{target} - Actual": "Average Actual Temp",
-        f"{target} - Error %": "Mean Error %"
-    }).reset_index()
-    hourly["Target"] = target
-    summary_rows.append(hourly)
+df_result.to_csv("boiler_multitarget_predictions.csv", index=False)
 
-summary_df = pd.concat(summary_rows, ignore_index=True)
-
-# Export results
-df_result.drop(columns=["hour"]).to_csv("boiler_multitarget_predictions.csv", index=False)
-summary_df.to_csv("boiler_multitarget_summary.csv", index=False)
-
-print("‚úÖ Model saved as boiler_temperature_multitarget.h5")
-print("üìÅ CSV files created: boiler_multitarget_predictions.csv + boiler_multitarget_summary.csv")
+# === 18. Report runtime
+end_time = time.time()
+print("‚úÖ Model saved as boiler_temperature_multitarget_lstm6h.h5")
+print("üìÅ Predictions saved to boiler_multitarget_predictions.csv")
+print(f"üïí Total training time: {end_time - start_time:.2f} seconds")
\ No newline at end of file
Index: UTILS/modifyData.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import pandas as pd\r\nimport random\r\nfrom datetime import time\r\n\r\n# File paths\r\ninput_file = \"weather_hourly_2024.csv\"\r\noutput_file = \"Updated_With_Boiler_Hourly_Realistic_v4.csv.gz\"\r\n##TEST##\r\n# Constants\r\nBOILER_SIZES = [50, 100, 150]\r\nENERGY_PER_DEGREE_PER_LITER = 1.16 / 1000  # kWh per liter per ¬∞C\r\nMAX_RADIATION = 1000  # radiation normalizations\r\nHEATING_START = time(18, 0)\r\nHEATING_END = time(21, 0)\r\nMAX_TEMP_SOLAR = 60\r\nMAX_TEMP_NO_SOLAR = 50\r\n\r\n# Init\r\nchunk_size = 100000\r\nupdated_chunks = []\r\nprevious_temps = {\r\n    size: {\"with\": random.uniform(30, 35), \"without\": random.uniform(30, 35)}\r\n    for size in BOILER_SIZES\r\n}\r\n\r\nfor chunk in pd.read_csv(input_file, chunksize=chunk_size, low_memory=False, on_bad_lines=\"warn\"):\r\n    chunk[\"date\"] = pd.to_datetime(chunk[\"date\"], errors=\"coerce\")\r\n    chunk = chunk.dropna(subset=[\"date\"])\r\n\r\n    for size in BOILER_SIZES:\r\n        chunk[f\"boiler temp for {size} L with solar system\"] = 0.0\r\n        chunk[f\"boiler temp for {size} L without solar system\"] = 0.0\r\n        chunk[f\"energy consumption for {size}L boiler with solar system\"] = 0.0\r\n        chunk[f\"energy consumption for {size}L boiler without solar system\"] = 0.0\r\n\r\n    for i in range(len(chunk)):\r\n        row = chunk.iloc[i]\r\n        ambient_temp = row[\"temperature_2m\"]\r\n        radiation = row.get(\"direct_radiation\", 0)\r\n        cloud_cover = row.get(\"cloud_cover\", 0.0) / 100\r\n        radiation_norm = min(radiation / MAX_RADIATION, 1.0)\r\n        current_time = row[\"date\"].time()\r\n        is_heating_time = HEATING_START <= current_time < HEATING_END\r\n        is_day = row.get(\"is_day\", 0) == 1\r\n\r\n        for size in BOILER_SIZES:\r\n            prev_without = previous_temps[size][\"without\"]\r\n            prev_with = previous_temps[size][\"with\"]\r\n\r\n            def add_noise(base, level=0.3):\r\n                return base + random.uniform(-level, level)\r\n\r\n            # --- WITHOUT solar system ---\r\n            min_temp_no_solar = 0.6 * ambient_temp\r\n            temp_without = prev_without\r\n\r\n\r\n            delta_env = ambient_temp - temp_without\r\n            temp_without += 0.08 * delta_env + random.uniform(-0.1, 0.1)\r\n\r\n            temp_without = max(min(add_noise(temp_without, 0.15), MAX_TEMP_NO_SOLAR), min_temp_no_solar)\r\n            previous_temps[size][\"without\"] = temp_without\r\n\r\n            # --- WITH solar system ---\r\n            temp_with = prev_with\r\n            size_modifier = 1 - (size - 50) / 200\r\n            solar_gain = radiation_norm * (1 - cloud_cover) * size_modifier\r\n\r\n\r\n            if is_day and radiation > 0:\r\n                temp_with += solar_gain * 5\r\n            else:\r\n                delta_env = ambient_temp - temp_with\r\n                temp_with += 0.08 * delta_env + random.uniform(-0.1, 0.1)\r\n\r\n            if radiation > 0:\r\n                temp_with = max(temp_with, temp_without + 0.5)\r\n\r\n            temp_with = max(min(add_noise(temp_with, 0.15), MAX_TEMP_SOLAR), 0.6 * ambient_temp)\r\n            previous_temps[size][\"with\"] = temp_with\r\n\r\n            # Energy consumption\r\n            energy_without = 0.0\r\n            energy_with = 0.0\r\n\r\n            chunk.at[chunk.index[i], f\"boiler temp for {size} L without solar system\"] = round(temp_without, 2)\r\n            chunk.at[chunk.index[i], f\"boiler temp for {size} L with solar system\"] = round(temp_with, 2)\r\n            chunk.at[chunk.index[i], f\"energy consumption for {size}L boiler without solar system\"] = round(energy_without, 4)\r\n            chunk.at[chunk.index[i], f\"energy consumption for {size}L boiler with solar system\"] = round(energy_with, 4)\r\n\r\n    updated_chunks.append(chunk)\r\n\r\n# Save\r\nprint(\"‚úÖ Start running!\")\r\nif updated_chunks:\r\n    df_final = pd.concat(updated_chunks, ignore_index=True)\r\n    numeric_columns = df_final.select_dtypes(include=[\"number\"]).columns\r\n    df_final[numeric_columns] = df_final[numeric_columns].round(4)\r\n    df_final.to_csv(output_file, index=False, compression=\"gzip\", float_format=\"%.4f\")\r\n    print(f\"‚úÖ The updated dataset has been saved to: {output_file}\")\r\nelse:\r\n    print(\"‚ùå No valid data to save.\")
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/UTILS/modifyData.py b/UTILS/modifyData.py
--- a/UTILS/modifyData.py	(revision 962acd81f45087c04b5cf28a31575b8ba5585d4d)
+++ b/UTILS/modifyData.py	(date 1744288445519)
@@ -10,8 +10,6 @@
 BOILER_SIZES = [50, 100, 150]
 ENERGY_PER_DEGREE_PER_LITER = 1.16 / 1000  # kWh per liter per ¬∞C
 MAX_RADIATION = 1000  # radiation normalizations
-HEATING_START = time(18, 0)
-HEATING_END = time(21, 0)
 MAX_TEMP_SOLAR = 60
 MAX_TEMP_NO_SOLAR = 50
 
@@ -40,7 +38,6 @@
         cloud_cover = row.get("cloud_cover", 0.0) / 100
         radiation_norm = min(radiation / MAX_RADIATION, 1.0)
         current_time = row["date"].time()
-        is_heating_time = HEATING_START <= current_time < HEATING_END
         is_day = row.get("is_day", 0) == 1
 
         for size in BOILER_SIZES:
