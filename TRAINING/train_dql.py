import numpy as np
import os
import matplotlib.pyplot as plt
import json
from tensorflow.keras.models import load_model
from tensorflow.keras.optimizers import Adam
from SIM.BoilerSimulator import BoilerSimulator
from AGENTS.dql_agent import DQLAgent
from tqdm import trange
import pandas as pd

# --- Load forecasted outside temperature (optional) ---
weather_forecast = pd.Series(np.random.uniform(10, 30, size=24*7))

# --- Define environment settings ---
boiler_capacity = 100
heater_power_kw = 3.0
has_solar = True
num_users = 2

env = BoilerSimulator(
    boiler_capacity_liters=boiler_capacity,
    heater_power_kw=heater_power_kw,
    has_solar=has_solar,
    num_users=num_users,
    weather_forecast=weather_forecast
)

state_size = env._get_state().shape[0]
action_size = 2

agent = DQLAgent(state_size=state_size, action_size=action_size)

model_path = 'dql_boiler_model.h5'
training_state_path = 'training_state.json'

start_episode = 0
best_reward = -np.inf
no_improvement_counter = 0
patience = 30  # Early stopping after 30 episodes without improvement

# --- Load existing model and training state ---
if os.path.exists(model_path):
    print("üìÇ Existing model found! Loading...")
    agent.model = load_model(model_path, compile=False)
    agent.model.compile(loss='mse', optimizer=Adam(learning_rate=0.0005))
    agent.update_target_model()

    if os.path.exists(training_state_path):
        with open(training_state_path, 'r') as f:
            state_data = json.load(f)
            start_episode = state_data.get('episodes_trained', 0)
            agent.epsilon = state_data.get('epsilon', 1.0)
        print(f"‚Ü∫ Continuing training from episode {start_episode} with epsilon {agent.epsilon:.2f}")
else:
    print("üÜï Starting fresh training.")

# --- Training settings ---
n_episodes = 300
episode_length = 24*3  # ◊®◊ß 3 ◊ô◊û◊ô◊ù ◊ú◊§◊®◊ß ◊ë◊û◊ß◊ï◊ù 7 ◊ô◊û◊ô◊ù (◊î◊ê◊¶◊î!)
batch_size = 32
rewards_per_episode = []

# --- Training loop ---
for e in trange(start_episode, n_episodes, desc="Training episodes"):
    random_target_temp = np.random.randint(60, 70)  # ◊ô◊ï◊™◊® ◊ï◊®◊ô◊ê◊¶◊ô◊î ◊ë◊ò◊û◊§'
    state = env.reset(target_temp=random_target_temp)
    total_episode_reward = 0

    for time in range(episode_length):
        action = agent.act(state)
        next_state, reward, done = env.step(action)
        agent.remember(state, action, reward, next_state, done)
        state = next_state
        total_episode_reward += reward

        if time % 10 == 0:  # Replay ◊®◊ß ◊õ◊ú 10 ◊¶◊¢◊ì◊ô◊ù
            agent.replay(batch_size)

    if e % 5 == 0:
        agent.update_target_model()

    rewards_per_episode.append(total_episode_reward)

    if (e + 1) % 10 == 0:  # Save model and training state ◊®◊ß ◊õ◊ú 10 ◊ê◊§◊ô◊ñ◊ï◊ì◊ï◊™
        agent.model.save(model_path)
        with open(training_state_path, 'w') as f:
            json.dump({
                "episodes_trained": e + 1,
                "epsilon": agent.epsilon
            }, f)

    # Track best reward
    if total_episode_reward > best_reward:
        best_reward = total_episode_reward
        no_improvement_counter = 0
        print(f"‚≠ê New best reward: {best_reward:.2f} at episode {e+1}")
    else:
        no_improvement_counter += 1

    if no_improvement_counter >= patience:
        print(f"‚èπ Early stopping triggered after {patience} episodes without improvement!")
        break

    if (e + 1) % 5 == 0:
        print(f"Episode {e+1}/{n_episodes} - Total Reward: {total_episode_reward:.2f} - Epsilon: {agent.epsilon:.2f}")

print(f"‚úÖ Training finished successfully at episode {e+1}!")

# --- Plot rewards ---
plt.plot(rewards_per_episode)
plt.title("Total Reward per Episode")
plt.xlabel("Episode")
plt.ylabel("Total Reward")
plt.grid()
plt.show()